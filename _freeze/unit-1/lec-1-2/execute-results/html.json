{
  "hash": "52cccf42afb765a80a72949066242438",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unit 1-2: Statistical Conclusion Validity\"\nsubtitle: \"Lecture Notes\"\nauthor: \"Sean Sylvia, Ph.D.\"\ndate: February 5, 2025\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n    code-tools: true\ndraft: false\n---\n\n\n\n# Summary\n\nThis lecture delves into statistical conclusion validity, exploring how we can assess the reliability and accuracy of estimated treatment effects in experimental research. As we have seen, due to the fundamental nature of potential outcomes, it is inherently impossible to recover individual treatment effects since each unit reveals only one potential outcome. However, under specific assumptions and with an appropriate assignment mechanism—such as random assignment—we can consistently estimate an average treatment effect (ATE). This lecture provides a brief, selective statistical background on how to estimate the ATE and quantify the uncertainty in these estimates, ensuring statistical conclusion validity. To streamline the presentation, we focus on settings with a binary treatment and largely ignore the role of covariates.\n\n# Learning Objectives\n\n1.  Explain the concept of statistical conclusion validity and its importance in ensuring that the estimated treatment effect reflects the true effect.\n2.  Identify the key sources of uncertainty in experimental studies, including sampling variation, variance in potential outcomes, and measurement error.\n3.  Review the principles of hypothesis testing, including the formulation of null and alternative hypotheses and the implications of Type I and Type II errors.\n4.  Explore the role of statistical power in determining the sample size needed to detect significant effects.\n5.  Explore the use of simulation techniques to conduct power calculations and assess the reliability of experimental designs.\n6.  Explore the role of multiple hypothesis testing and its implications in statistical inference.\n\n# Uncertainty and Statistical Conclusion Validity\n\n## What is Uncertainty?\n\nUncertainty in empirical research refers to the inherent imprecision that arises when we attempt to infer quantities that cannot be directly observed. Whether we are engaged in descriptive, causal, or generalization inference, our estimates come with uncertainty that must be quantified and communicated.\n\nTwo primary frameworks exist for this purpose: the Bayesian and the frequentist approaches.[^1]\n\n[^1]: Reference: [Research Design in the Social Sciences](https://book.declaredesign.org/declaration-diagnosis-redesign/choosing-answer-strategy.html#uncertainty)\n\n### Bayesian Approach\n\nThe Bayesian framework uses Bayes’ rule to combine prior beliefs with the observed data, resulting in a posterior probability distribution over the parameter of interest, $\\theta$. Mathematically, this is expressed as:\n\n$$\n\\Pr(\\theta = \\theta' \\mid d = d') = \\frac{\\Pr(d = d' \\mid \\theta = \\theta')\\, \\Pr(\\theta = \\theta')}{\\sum_{\\theta''} \\Pr(d = d' \\mid \\theta = \\theta'')\\, \\Pr(\\theta = \\theta'')},\n$$\n\nwhere:\n\n-   $d$ represents data, and $d'$ represents the observed data (or an \"observed realization of the data\")\n\n-   $\\theta'$ and $\\theta''$ represent particular values of the parameter $\\theta$.\n\nFrom this posterior distribution, the **posterior mean** serves as our best estimate of $\\theta$, and the **posterior variance** quantifies the uncertainty associated with that estimate.\n\nBy applying Bayes’ rule over different values of $\\theta$, we construct a complete probability distribution that represents all possible answers. This posterior distribution simultaneously provides our best estimate—often summarized by the posterior mean—and quantifies our uncertainty via the posterior variance.\n\nWhile intuitive, a challenge is that specifying prior uncertainty ($\\Pr(\\theta = \\theta')$) is often a subjective choice, and the posterior distribution is often difficult to interpret and communicate.\n\n### Frequentist Approach\n\nIn contrast, the frequentist approach avoids specifying prior beliefs and focuses on the likelihood function, $\\Pr(d = d' \\mid \\theta = \\theta')$, which describes the probability of observing the data $d'$ given a specific value of $\\theta$.[^2] I.e. instead of thinking of the strength of beliefs, we consider that $\\theta$ generates the actual probability distriubtion over possible data $d$.\n\n[^2]: Can you explain in words how this differs from the Bayesian probability above?\n\nThis approach yields useful quantities:\n\n**P-value**: The p-value for a null hypothesis, $\\theta = \\theta_0$, is defined as the probability of observing data as extreme as $d_{m*}$ under the null hypothesis, or\n\n$$\n\\Pr(d = d_{m*} \\mid \\theta = \\theta_0).\n$$\n\nwhere $d_{m*}$ is the test statistic.\n\n**Confidence Interval**: A 95% confidence interval is constructed such that, if the experiment were repeated many times, 95% of the intervals would contain the true parameter value. This approach provides a framework to rule out parameter values that are inconsistent with the observed data, or $Pr(d = d' \\mid \\theta = \\theta') \\leq 0.05$.\n\n## Where Does Uncertainty Come From in an Experimental Study?\n\nBefore getting into estimation and the uncertainty of the estimate, we need to be precise about the source of uncertainty. In experimental studies, uncertainty is an inherent part of the inference process, arising from several key sources. Recognizing these sources is critical to designing robust experiments and correctly interpreting the results. The main contributors to uncertainty include:\n\n-   **Sampling Variation:**\\\n    Uncertainty due to sampling variation stems from the fact that any sample drawn from a population is just one of many possible samples. Consequently, the same treatment might yield different results if applied to a different sample, reflecting random fluctuations in the selection process.\n\n-   **Variance in Potential Outcomes:**\\\n    The natural variability in the potential outcomes (i.e., the outcomes that would be observed under different treatment conditions) can lead to uncertainty. High variance makes it more challenging to detect a true treatment effect because the noise in the data can obscure the signal, thereby reducing the study's power to reject a false null hypothesis.\n\n-   **Measurement Error:**\\\n    Measurement error occurs when there are inaccuracies in recording or assessing the potential outcomes. Such errors introduce additional variability and can bias the estimated treatment effect, further contributing to uncertainty in the experimental results.\n\nTo vizualize this, the diagram below shows a \"directed acyclic graph\" (DAG) representation of the \"data strategy\" framework discussed in [Chapter 8 of Research Design in the Social Sciences](https://book.declaredesign.org/declaration-diagnosis-redesign/crafting-data-strategy.html).\n\n![Data Strategy DAG, Source: Research Design in the Social Sciences](/unit-1/media/figure-8-1.svg)\n\n> In Figure 8.1, we illustrate these three elements of data strategies: sampling (S), treatment assignment (Z), and measurement (Q). These nodes are highlighted by blue boxes to emphasize that they are in the control of the researcher. No arrows go into the S, Z, or Q nodes; they are set by the researcher. In each case, the strategy selected by the researcher affects a corresponding endogenous variable. The sampling procedure causes changes in the endogenous response (R), which represents whether participants provide outcome data, for example responding to survey questions. R is not under the full control of the researchers: it is affected by S, the sampling procedure, but also by the idiosyncratic choices of participants who have higher and lower interest and ability to respond and participate in the study (U). Similarly, the endogenous variable treatment D represents whether participants actually receive the treatment, regardless of their assignment Z. D is affected by the treatment assignment procedure (Z) of course. But except in cases when Z fully determines D (no noncompliance), we are concerned that it will be affected by unobserved idiosyncratic features of individuals U. The third researcher node is Q, the measurement procedure. Q affects Y, the observed outcome, measured by the researcher. Y is also affected by a latent variable Y\\*, which cannot be directly observed. The measurement procedure provides an imperfect measurement of that latent variable, which is (potentially) affected by treatment D and unobserved heterogeneity U. In the robustness section at the end of the chapter, we explore further variations of this DAG that incorporate threats to inference from noncompliance, attrition, excludability violations, and interference.\n\n::: callout-tip\nCan you draw four arrows representing the four exclusion restrictions?\n:::\n\n## Statistical Conclusion Validity\n\nGiven that our exclusion restrictions are satisfied, we can estimate the average treatment effect (ATE). The question is then: How can we ensure valid statistical conclusions from our estimate of the ATE?\n\nWe need to consider the uncertainty in our estimate, and whether we can reject the null hypothesis that the treatment has no effect.\n\n# **Sampling Frameworks: Understanding the Foundations of Estimation in Experiments**\n\nWhen designing an experiment, we need to consider how the sample relates to the broader population of interest. This decision influences the **statistical inference** and generalizability of findings. Two main perspectives in sampling frameworks help frame this discussion: the **super-population approach** and the **finite-population approach**.\n\n-   **super-population approach**: units are assumed an independent sample from some hypothetical infinite population.\n-   **finite-population approach**: inference is restricted to the specific individuals in the sample. Potential outcomes of the experimental units are fixed, and the randomness comes solely from the treatment assignment.\n\n## **The Super-Population Approach**\n\nThe **super-population approach** assumes that the study sample is drawn from a larger, hypothetical **infinite population** represented by a probability distribution, $Q$. This perspective views potential outcomes—both with and without treatment—as stochastic variables drawn from $Q$. The primary goal in this framework is to estimate a **feature of this distribution**, typically the **expected treatment effect**:\n\n$$\nE[Y(1) - Y(0)]\n$$\n\nwhere $Y(1)$ and $Y(0)$ denote the potential outcomes under treatment and control conditions, respectively. Under this approach, each sample is considered an independent and identically distributed (i.i.d.) draw from the distribution $Q$, meaning the researcher is interested in making **generalizable inferences** beyond the study sample.\n\nA key implication of this framework is that **two sources of variance** affect our estimation of treatment effects: 1. **Sampling variance**—arising from differences between one sample and another. 2. **Assignment mechanism variance**—introduced by the randomness in treatment assignment.\n\nThe super-population approach is useful when researchers aim to extend their findings to a broader population, such as in **policy recommendations** or **clinical trials**. However, it requires strong assumptions about how well the study sample represents the population.\n\n## **The Finite-Population Approach**\n\nIn contrast, the **finite-population approach** considers the sample as a fixed, well-defined group rather than a subset of an infinite population. Here, the researcher is not making inferences beyond the observed sample but instead treating the units as the **entire relevant population**. This approach is common in **evaluations of specific interventions** where the focus is on estimating the **finite-population average treatment effect (ATE)** (also called the sample average treatemnt effect, or SATE):[^3]\n\n[^3]: [Wager Chapter 1](https://web.stanford.edu/~swager/causal_inf_book.pdf) refers to this as SATE.\n\n$$\n\\tau_{fp} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[Y_i(1) - Y_i(0)\\right]\n$$\n\nwhere $N$ is the total number of units in the study. Unlike in the super-population approach, the potential outcomes in a finite population framework are **fixed, not random**. The treatment effect is then viewed as an empirical quantity to be estimated within the sample, rather than a parameter of an underlying distribution.\n\nA **practical distinction** between the two approaches is in their implications for statistical inference:\n\n-   In the **super-population approach**, standard errors reflect both **sampling variability** and **randomization-induced variation**.\n\n-   In the **finite-population approach**, standard errors are based only on the **variation within the observed sample**, without assuming a broader distribution.\n\nThis framework is particularly relevant when researchers are concerned with **internal validity** over generalizability, such as in **program evaluations or field experiments**.\n\n## **Subpopulations in the Super-Population Framework**\n\nWithin the super-population framework, researchers often refine their analysis by considering **subpopulations** to account for **heterogeneous treatment effects**. One important example is the **Conditional Average Treatment Effect (CATE)**:\n\n$$\nE[Y(1, X) - Y(0, X) | X]\n$$\n\nwhere $X$ represents observed covariates that influence treatment effects. This approach allows for **differentiated insights** across groups, such as demographic segments in public health interventions.\n\n## **Choosing Between the Two Approaches**\n\nThe choice between these sampling frameworks depends on the **research question**:\n\n-   If the goal is to make **generalizable claims** about a broader population, the **super-population approach** is preferred.\n-   If the study focuses on **a specific, finite group of units**, the **finite-population approach** is more appropriate.\n\nBoth perspectives provide valuable insights, and many empirical studies incorporate elements of both frameworks, particularly when considering **external validity and policy relevance**.\n\n# Single Hypothesis Testing and Statistical Power\n\nWith our sampling framework in mind, we can now turn to **single hypothesis testing** and **statistical power**. The first thing we need to do is choose an **estimator** of interest. If the estimand is an **Average Treatment Effect (ATE)**, then a reasonable choice is the **difference-in-means estimator**. This estimator compares the mean outcome of the treated group ($Y(1)$) and the control group ($Y(0)$):\n\n$$\n\\hat{\\tau} = \\bar{Y}_1 - \\bar{Y}_0\n$$\n\nwhere $\\bar{Y}_1$ and $\\bar{Y}_0$ are the sample means of the treated and control groups, respectively. The difference-in-means estimator provides an unbiased estimate of the **Average Treatment Effect (ATE)** (the estimand) under the assumption of random assignment.\n\nHowever, while the estimator provides a point estimate of the treatment effect, it does not convey **uncertainty**. To formally assess whether the estimated effect is significantly different from zero, we conduct a **statistical hypothesis test**.\n\nThe fundamental question we seek to answer is:\\\n**Can we be confident in detecting the reported effect size in our experimental results?**\n\nTo do so, we frame our problem in terms of **hypothesis testing**:\n\n-   We begin by specifying a **null hypothesis (**$H_0$), which typically asserts that the treatment has no effect ($\\tau = 0$).\n\n-   The **alternative hypothesis (**$H_A$) posits that there is a nonzero effect ($\\tau \\neq 0$).\n\n-   Hypothesis testing allows us to assess whether the observed difference-in-means provides sufficient evidence to reject $H_0$.\n\nThe hypothesis test is based on the **sampling distribution** of the estimator. Because we assume random assignment, the **difference-in-means estimator follows a known distribution**, and we use this fact to determine whether the observed effect is large enough to be statistically significant.\n\n## Statistical Significance and the t-Statistic\n\nTo formally test the null hypothesis, we construct a **t-statistic**:\n\n$$\nt = \\frac{\\hat{\\tau}}{\\text{SE}(\\hat{\\tau})}\n$$\n\nwhere **SE(**$\\hat{\\tau}$) represents the standard error of the difference-in-means estimator. As the sample size grows, this t-statistic follows a standard normal distribution (or Student's t-distribution for small samples). A large absolute value of \\$ t \\$ provides evidence against the null hypothesis.\n\nTo determine whether the result is **statistically significant**, we compare the t-statistic to a **critical value** determined by our chosen **significance level** ($\\alpha$, commonly set at 0.05). If the absolute value of the t-statistic exceeds this threshold, we reject the null hypothesis.\n\n## Type I and Type II Errors\n\nWhile hypothesis testing provides a structured approach to evaluating treatment effects, errors can still occur:\n\n1.  **Type I Error (**$\\alpha$): Rejecting the null hypothesis when it is actually true (false positive).\n    -   Controlled by setting the significance level ($\\alpha$), which determines the probability of mistakenly rejecting $H_0$.\n    -   Lower $\\alpha$ reduces false positives but increases the risk of missing real effects.\n2.  **Type II Error (**$\\beta$): Failing to reject the null hypothesis when it is actually false (false negative).\n    -   Related to **statistical power**, which is the probability of detecting an effect when it truly exists.\n\n![Type 1 and Type 2 Errors](/unit-1/media/HypothesisTesting.png)\n\n## Power Calculation: Ensuring Detectability\n\n**Statistical power** refers to the ability of a test to correctly reject the null hypothesis when a true effect exists. Mathematically:\n\n$$\n\\text{Power} = 1 - \\beta\n$$\n\nFactors influencing power:\n\n-   **Effect size (**$\\tau$): Larger effects are easier to detect.\n-   **Sample size (**$N$): Larger samples reduce variability, increasing power.\n-   **Significance level (**$\\alpha$): Lowering $\\alpha$ increases the risk of missing true effects.\n-   **Standard deviation of outcomes**: Higher variability in outcomes reduces power.\n\nTo achieve a well-powered experiment, researchers conduct **power calculations** before data collection to determine the minimum sample size required to detect an effect with reasonable confidence.\n\n### A Simple Example\n\nLet's consider a simple example of a power simulation using a simple random assignment to a treatment and control group, where the estimate is the **difference-in-means estimator**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\n```\n:::\n\n\n\n#### **Step 1: Define Simulation Parameters**\n\n-   Let’s assume we have a total sample size of $N = 200$ individuals.\n-   The treatment group receives an intervention, while the control group does not.\n-   The true treatment effect is set to $\\tau = 2$.\n-   The outcome variable follows a normal distribution with mean 10 and standard deviation 4.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(072111)  # Ensures reproducibility\n\n# Define parameters\nN <- 200  # Total sample size\np <- 0.5  # Probability of assignment to treatment\ntrue_tau <- 2  # True treatment effect\nsigma <- 4  # Standard deviation of outcome\n```\n:::\n\n\n\n#### **Step 2: Simulate Data**\n\nNow we can simulate the data using data.table:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate data using data.table\ndt <- data.table(id = 1:N)\ndt[, treatment := rbinom(.N, 1, p)]\ndt[, outcome := 10 + true_tau * treatment + rnorm(.N, mean = 0, sd = sigma)]\n\n#display first few rows\nhead(dt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      id treatment  outcome\n   <int>     <int>    <num>\n1:     1         0 10.39758\n2:     2         1 17.78058\n3:     3         0 11.46925\n4:     4         0 14.56752\n5:     5         1 10.55347\n6:     6         1 14.99933\n```\n\n\n:::\n:::\n\n\n\n#### **Step 3: Estimate Treatment Effect**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff_means <- dt[treatment == 1, mean(outcome)] - dt[treatment == 0, mean(outcome)]\nSE <- sqrt(dt[treatment == 1, var(outcome)] / dt[treatment == 1, .N] +\n           dt[treatment == 0, var(outcome)] / dt[treatment == 0, .N])\n\nt_stat <- diff_means / SE  # Compute t-statistic\np_value <- 2 * (1 - pt(abs(t_stat), df = N - 2))  # Two-tailed test\n\n# Display results\ncat(\"Estimated Treatment Effect:\", diff_means, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimated Treatment Effect: 2.453868 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"p-value:\", p_value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\np-value: 1.088986e-05 \n```\n\n\n:::\n:::\n\n\n\n#### **Step 4: Power Simulation**\n\nFirst, let's consider how we will interpret the results:\n\n-   If the p-value is **less than 0.05**, we reject the null hypothesis and conclude that the treatment has a significant effect.\n-   If the p-value is **greater than 0.05**, we fail to reject the null, meaning we do not have enough evidence to confirm a treatment effect.\n\nNow, let's simulate a power simulation:\n\nFirst, define the power simulation function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_power <- function(N, true_tau, sigma, p, alpha, reps = 1000) {\n  rejections <- 0\n  \n  for (i in 1:reps) {\n    dt <- data.table(id = 1:N)\n    dt[, treatment := rbinom(.N, 1, p)]\n    dt[, outcome := 10 + true_tau * treatment + rnorm(.N, mean = 0, sd = sigma)]\n    diff_means <- dt[treatment == 1, mean(outcome)] - dt[treatment == 0, mean(outcome)]\n    SE <- sqrt(dt[treatment == 1, var(outcome)] / dt[treatment == 1, .N] +\n               dt[treatment == 0, var(outcome)] / dt[treatment == 0, .N])\n    t_stat <- diff_means / SE\n    p_value <- 2 * (1 - pt(abs(t_stat), df = N - 2))\n    \n    if (p_value < alpha) {\n      rejections <- rejections + 1\n    }\n  }\n  return(rejections / reps)\n}\n```\n:::\n\n\n\nNow, simulate this experiment **1,000 times**:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run power simulation\npower <- simulate_power(N = 200, true_tau = 2, sigma = 4, p = 0.5, alpha = 0.05, reps = 1000)\ncat(\"Estimated Power:\", power, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimated Power: 0.938 \n```\n\n\n:::\n:::\n\n\n\n### **Step 5: Interpreting Power Calculation**\n\n-   The **power** of the test is the proportion of simulations in which we correctly reject the null hypothesis when the treatment effect is truly $\\tau = 2$.\n-   A power value close to **0.80 or higher** indicates that the experiment is well-powered.\n\n::: callout-tip\nTry changing the sample size or effect size, to see how the power changes.\n:::\n\n# Simulations in Research Design\n\nTo fix ideas of what we're doing with simulations, it might be helpful to take a step back and view this in the context of an overall research design.\n\nWe aim to generate empirical answers that closely approximate the true treatment effect. However, because we never observe both potential outcomes for an individual, we rely on statistical methods and experimental design to make **valid inferences**. Simulation plays a crucial role in assessing research designs, allowing us to compare **simulated answers** to possible true answers across multiple realizations of a study.\n\nIn empirical research, a design consists of two halves:\n\n1.  **Theoretical Component**: Defines the inquiry using a model $M$.\n2.  **Empirical Component**: Applies the data strategy $D$ to generate a dataset, then applies an estimation strategy $A$ to obtain the empirical answer $a_d$.\n\nSimulation dissociates the design from reality, allowing researchers to assess how their chosen model performs across various possible scenarios, even when the true answer is unknown.\n\n## The MIDA Framework for Simulation\n\nA structured way to think about research design and simulation is the **MIDA Framework**,[^4](https://book.declaredesign.org/declaration-diagnosis-redesign/research-design.html)\\] which consists of:\n\n[^4]: Reference: Declaration Design\n\n-   **M**: The Model - The underlying data-generating process that defines the inquiry.\n-   **I**: The Inquiry - The specific research question we are trying to answer.\n-   **D**: The Data Strategy - The way we collect and structure data, including sampling and treatment assignment.\n-   **A**: The Answer Strategy - The statistical method we use to estimate the effect.\n\n![MIDA Framework, Source: Declaration Design](/unit-1/media/MIDA.svg)\n\n![Elements of Research Design, Source: Declaration Design](/unit-1/media/RDElements.png)\n\nIn a real-world study, we can only observe a single realization of the design and generate one empirical answer $a_d$. However, through **simulation**, we can consider many possible data realizations by repeatedly drawing from different models $m_1, m_2, ..., m_k$ within the model space $M$. This allows us to assess how our research design performs across different scenarios.\n\n## Diagnosing Research Designs with Simulation\n\nWhen we simulate a research design, we evaluate its **performance** by considering:\n\n-   **Bias**: How close is the empirical answer $a_d$ to the true answer $a_m$?\n-   **Variance**: How much do the empirical answers fluctuate across different realizations?\n-   **Coverage**: How often do confidence intervals include the true effect?\n-   **Power**: How frequently does the study correctly reject the null hypothesis when a true effect exists?\n\nThe bottom half of the figure below illustrates how simulation allows us to examine the research design across multiple models ($m_1, ..., m_k$), generating different answers ($a_{m_1}, a_{m_2}, ..., a_{m_k}$) and associated datasets ($d_1, d_2, ..., d_k$). The simulated research design does not have direct access to the true answer but can assess performance across the models under consideration.\n\n![Simulations in the MIDA Framework](/unit-1/media/MIDA-Simulation.svg)\n\n# Multiple Hypothesis Testing\n\n## The Challenge of Multiple Hypothesis Testing\n\nIn many empirical settings, researchers conduct **multiple hypothesis tests** rather than a single test. This introduces the risk of **false positives**, or mistakenly finding significant effects simply due to chance. Multiple hypothesis testing arises naturally in at least three key scenarios:\n\n1.  **Multiple Outcomes**: When we examine several outcomes ($Y_i$) to determine whether any are affected by the treatment.\n2.  **Heterogeneous Treatment Effects (CATEs)**: When treatment effects vary across subgroups, and we want to assess which subgroups exhibit an effect.\n3.  **Multiple Treatments**: When we compare multiple interventions ($D_i$) and want to test their effects relative to a control group or to each other.\n\nTo properly interpret results, we need statistical techniques that **adjust for multiple comparisons** and control the probability of **false discoveries**.\n\n------------------------------------------------------------------------\n\n## Types of Multiple Hypothesis Tests\n\nThere are two broad types of statistical hypothesis testing frameworks when dealing with multiple comparisons:\n\n-   **Joint Tests**: These assess whether at least one hypothesis is true (e.g., \"Is at least one treatment effective?\").\n-   **Simultaneous Tests**: These examine whether multiple hypotheses are true at the same time (e.g., \"Are both Treatment A and Treatment B effective?\").\n\nWhen conducting multiple tests, researchers need to control for an increased **family-wise error rate (FWER)**, which is the probability of making at least one Type I error (false positive).\n\n------------------------------------------------------------------------\n\n## Controlling the Family-Wise Error Rate (FWER)\n\nA single hypothesis test controls the probability of a **Type I error** at a given significance level ($\\alpha$). However, with multiple tests, the probability of making **at least one false rejection** increases:\n\n$$\n\\text{FWER} = 1 - (1 - \\alpha)^k\n$$\n\nwhere $k$ is the number of tests. For example, if we conduct **5 tests at** $\\alpha = 0.05$, the probability of making **no Type I errors** across all tests is:\n\n$$\n(1 - 0.05)^5 = 0.7738\n$$\n\nThus, the probability of making **at least one** Type I error is:\n\n$$\nFWER = 1 - 0.7738 = 0.2262\n$$\n\nThis means there is a **22.62% chance** of mistakenly rejecting at least one null hypothesis across the five tests. If we perform **20 tests**, the FWER increases to **64%**.\n\n------------------------------------------------------------------------\n\n## Approaches to Controlling the FWER\n\nTo mitigate this issue, researchers employ various **multiple testing corrections**, such as:\n\n-   **Bonferroni Correction**: Adjusts the significance level by dividing $\\alpha$ by the number of tests: $\\alpha^* = \\alpha / k$. This is simple but conservative.\n-   **Holm Method**: A stepwise procedure that ranks p-values and adjusts them sequentially to control the FWER more efficiently.\n-   **Modern Approaches (e.g., Westfall-Young, Benjamini-Hochberg FDR control)**: These methods control for false discovery rates and are widely used in large-scale testing.\n\n------------------------------------------------------------------------\n\n## Simulating Multiple Hypothesis Testing in R\n\nTo illustrate the impact of multiple testing and FWER correction, let's do a simulation in R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nset.seed(072111)  # Ensures reproducibility\n\n# Define parameters\nN <- 200  # Sample size\nk <- 10  # Number of hypothesis tests\nalpha <- 0.05  # Significance level\n\n# Simulate k independent hypothesis tests\ndt <- data.table(test_id = 1:k)\ndt[, p_value := runif(.N, min = 0, max = 1)]  # Generate uniform random p-values\n\ndt[, bonferroni := p_value < (alpha / k)]  # Bonferroni correction\ndt[, holm := p.adjust(p_value, method = \"holm\") < alpha]  # Holm correction\n\ndt[, naive_reject := p_value < alpha]  # Standard test (without correction)\n\n# Count false positives\nfalse_discoveries <- dt[, sum(naive_reject)]\nadjusted_false_discoveries <- dt[, sum(bonferroni)]\nholm_false_discoveries <- dt[, sum(holm)]\n\ncat(\"False positives (no correction):\", false_discoveries, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse positives (no correction): 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"False positives (Bonferroni correction):\", adjusted_false_discoveries, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse positives (Bonferroni correction): 0 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"False positives (Holm correction):\", holm_false_discoveries, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFalse positives (Holm correction): 0 \n```\n\n\n:::\n:::\n\n\n\n### **Interpreting the Simulation**\n\n-   **Without correction**, we expect around $\\alpha \\times k$ false discoveries.\n-   **Bonferroni correction** sharply reduces false positives but may be overly conservative.\n-   **Holm correction** provides a better balance, controlling FWER while maintaining statistical power.\n\n::: callout-tip\nNote: In the lab, we'll cover some modern approaches that are more powerful but a bit more complex.\n:::\n\n# References\n\n1.  [Wager Chapter 1](https://web.stanford.edu/~swager/causal_inf_book.pdf)\n2.  [Declare Design Book](https://book.declaredesign.org/declaration-diagnosis-redesign/declaring-designs.html)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}