{
  "hash": "01beed134e15339e8e3cad67afb93664",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Session 0.2: Reproducible Research & Computing Setup\"\nsubtitle: \"HPM 883: Advanced Quantitative Methods\"\nauthor: \"Sean Sylvia\"\ndate: \"January 12, 2026\"\nformat:\n  revealjs:\n    theme: default\n    slide-number: true\n    transition: fade\n    progress: true\n    chalkboard: true\n    smaller: true\n    scrollable: true\n    footer: \"HPM 883 | Session 0.2\"\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n## Today's Agenda {.center}\n\n1. Reproducible research principles\n2. Computing environment setup\n3. Introduction to DeclareDesign\n4. Simulating your first RCT\n5. Lab 0 overview\n\n::: {.notes}\nThis is a hands-on session. Have your laptops ready. We'll be coding together.\n:::\n\n---\n\n## Why Reproducibility Matters\n\n::: {.columns}\n::: {.column width=\"50%\"}\n### The Reproducibility Crisis\n\n- 70%+ of researchers have failed to reproduce another's work\n- 50%+ have failed to reproduce their *own* work\n- Retractions increasing exponentially\n\n:::\n\n::: {.column width=\"50%\"}\n### Our Solution\n\n- Version control (Git)\n- Package management (renv)\n- Pipeline automation (targets)\n- Literate programming (Quarto)\n\n:::\n:::\n\n::: {.callout-important}\n## Course Requirement\nAll submitted work must be reproducible. If we can't run your code, we can't grade it.\n:::\n\n---\n\n## The Reproducibility Stack\n\n```{mermaid}\n%%| fig-alt: \"Four layers of reproducibility: Quarto (reports) on top, targets (pipeline), renv (packages), Git (version control) at bottom\"\nflowchart TB\n    subgraph L4[\"Literate Programming\"]\n        Q[\"Quarto: Code + Narrative\"]\n    end\n\n    subgraph L3[\"Pipeline\"]\n        T[\"targets: Automated workflow\"]\n    end\n\n    subgraph L2[\"Environment\"]\n        R[\"renv: Package versions\"]\n    end\n\n    subgraph L1[\"Version Control\"]\n        G[\"Git: Track changes\"]\n    end\n\n    L4 --> L3\n    L3 --> L2\n    L2 --> L1\n```\n\n---\n\n## Git & GitHub\n\n### Why Version Control?\n\n- **Track changes**: See exactly what changed when\n- **Collaborate**: Multiple people, same codebase\n- **Backup**: Your work lives in the cloud\n- **Submission**: GitHub Classroom for assignments\n\n::: {.fragment}\n### Basic Workflow\n\n```bash\ngit clone <repo-url>    # Copy repo to local\ngit add .               # Stage changes\ngit commit -m \"message\" # Save snapshot\ngit push                # Upload to GitHub\n```\n:::\n\n---\n\n## Package Management with renv\n\n### The Problem\n\n```r\n# Works on your machine...\ninstall.packages(\"dplyr\")  # Version 1.1.4\n\n# Breaks on mine...\n# I have dplyr 0.8.3\n```\n\n::: {.fragment}\n### The Solution: renv\n\n```r\n# First time setup (already done in templates)\nrenv::init()\n\n# Restore packages (you run this)\nrenv::restore()\n\n# After adding packages\nrenv::snapshot()\n```\n:::\n\n---\n\n## Pipeline Automation with targets\n\n### Why targets?\n\n- Only re-run what changed\n- Document dependencies automatically\n- Scale to complex analyses\n\n::: {.fragment}\n### Basic Structure\n\n```r\n# _targets.R\nlist(\n  tar_target(raw_data, read_csv(\"data/raw.csv\")),\n  tar_target(clean_data, clean_fn(raw_data)),\n  tar_target(model, fit_model(clean_data)),\n  tar_target(results, summarize(model))\n)\n```\n:::\n\n::: {.callout-tip}\nWe'll use targets for the capstone project. Labs use simpler structure.\n:::\n\n---\n\n## Literate Programming with Quarto\n\n### Code + Narrative = Reproducible Report\n\n```{.markdown}\n## Methods\n\nWe estimate the treatment effect using:\n\n`​``{r}\nmodel <- lm_robust(outcome ~ treatment, data = df)\n`​``\n\nThe estimated effect is 0.42.\n```\n\n::: {.fragment}\n**Output:** A document where code, results, and interpretation live together.\n:::\n\n---\n\n## Your Computing Environment\n\n### Three Tiers\n\n| Tier | Platform | When |\n|------|----------|------|\n| In-Class | Posit Cloud | Today, lectures |\n| Projects | Positron/RStudio + GitHub | Labs, capstone |\n| Submission | GitHub Classroom | All graded work |\n\n::: {.callout-note}\n## Setup Guide\nSee [Student Setup Guide](../Resources/setup-guide.qmd) for detailed installation instructions.\n:::\n\n---\n\n## Live Demo: Posit Cloud\n\nLet's walk through:\n\n1. Log into posit.cloud\n2. Join HPM 883 workspace\n3. Open today's project\n4. Run `renv::restore()`\n5. Verify packages load\n\n::: {.notes}\nWalk through the demo slowly. Pause for questions. This is where students often get stuck.\n:::\n\n---\n\n## Introduction to DeclareDesign\n\n### A Grammar for Research Design\n\nDeclareDesign provides a unified framework:\n\n- **Model (M)**: Data generating process\n- **Inquiry (I)**: What we want to learn\n- **Data Strategy (D)**: How we collect data\n- **Answer Strategy (A)**: How we estimate\n\n```r\ndesign <- declare_model(...) +\n          declare_inquiry(...) +\n          declare_sampling(...) +\n          declare_assignment(...) +\n          declare_estimator(...)\n```\n\n---\n\n## The MIDA Framework\n\n```{mermaid}\n%%| fig-alt: \"MIDA framework showing Model, Inquiry, Data Strategy, and Answer Strategy as connected components\"\nflowchart LR\n    M[\"Model<br>(DGP)\"] --> I[\"Inquiry<br>(Estimand)\"]\n    I --> D[\"Data Strategy<br>(Sampling, Assignment)\"]\n    D --> A[\"Answer Strategy<br>(Estimator)\"]\n    A --> |\"Diagnosis\"| M\n```\n\n::: {.fragment}\n### Key Insight\n\nDesign declaration **before** data collection forces clear thinking about:\n\n- What we're trying to learn (estimand)\n- What assumptions we're making (model)\n- How we'll analyze the data (estimator)\n:::\n\n---\n\n## Example: Simple RCT\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(DeclareDesign)\nlibrary(tidyverse)\n\n# Declare the design\nsimple_rct <- declare_model(\n  N = 100,\n  U = rnorm(N),\n  potential_outcomes(Y ~ 0.5 * Z + U)\n) +\ndeclare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\ndeclare_assignment(Z = complete_ra(N)) +\ndeclare_measurement(Y = reveal_outcomes(Y ~ Z)) +\ndeclare_estimator(Y ~ Z, .method = lm_robust)\n```\n:::\n\n\n---\n\n## Running the Design\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run once\nrun_design(simple_rct) |>\n  select(inquiry, estimand, estimate, std.error) |>\n  head(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  inquiry estimand  estimate std.error\n1     ATE      0.5 0.3931238 0.2222809\n```\n\n\n:::\n:::\n\n\n::: {.fragment}\n### What just happened?\n\n1. Generated 100 units with potential outcomes\n2. Randomly assigned to treatment\n3. Revealed observed outcomes\n4. Estimated ATE with robust SEs\n:::\n\n---\n\n## Diagnosing the Design\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate 500 runs\ndiagnosis <- diagnose_design(simple_rct, sims = 500)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiagnosis |>\n  reshape_diagnosis() |>\n  select(Bias, RMSE, Power, Coverage)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Bias   RMSE  Power Coverage\n1  -0.00   0.20   0.70     0.95\n2 (0.01) (0.01) (0.02)   (0.01)\n```\n\n\n:::\n:::\n\n\n---\n\n## Interpretation\n\n| Metric | Value | Target |\n|--------|-------|--------|\n| **Bias** | ~0 | Near 0 |\n| **RMSE** | ~0.20 | Low |\n| **Power** | ~0.60 | ≥ 0.80 |\n| **Coverage** | ~0.95 | 0.95 |\n\n::: {.fragment}\n### Problem: Underpowered!\n\nWith N=100 and true effect of 0.5, we only have ~60% power.\n\n**Solutions:**\n- Increase sample size\n- Block on predictive covariates\n- Use CUPED/regression adjustment\n:::\n\n---\n\n## Your Turn: Modify the Design\n\nTry changing:\n\n1. `N = 200` — What happens to power?\n2. `potential_outcomes(Y ~ 0.3 * Z + U)` — Smaller effect?\n3. `complete_ra(N, prob = 0.3)` — Unequal allocation?\n\n```r\n# Template\nmy_design <- declare_model(\n  N = ???,\n  U = rnorm(N),\n  potential_outcomes(Y ~ ??? * Z + U)\n) +\ndeclare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +\ndeclare_assignment(Z = complete_ra(N, prob = ???)) +\ndeclare_measurement(Y = reveal_outcomes(Y ~ Z)) +\ndeclare_estimator(Y ~ Z, .method = lm_robust)\n```\n\n---\n\n## Lab 0: Environment Setup\n\n### Objectives\n\nVerify your computing environment works:\n\n1. Clone repo from GitHub Classroom\n2. Restore packages with renv\n3. Load and summarize data\n4. Run a simple regression\n5. Render to HTML and push to GitHub\n\n::: {.callout-important}\n## Due Date\n**Sunday, January 19** (end of Week 1)\n:::\n\n---\n\n## Lab 0 Walkthrough\n\n### Step 1: Accept Assignment\n\n1. Click GitHub Classroom link (in Slack)\n2. Accept assignment → creates your repo\n\n### Step 2: Clone Locally\n\n```bash\ngit clone git@github.com:unc-hpm-quant/lab-0-yourusername.git\n```\n\n### Step 3: Open & Restore\n\n1. Open `.Rproj` in Positron\n2. Run `renv::restore()` in console\n\n---\n\n## Getting Help\n\n### When Stuck\n\n1. **Slack #hpm883-help** — Quick questions\n2. **Office Hours** — Wednesday 2-4pm\n3. **Email** — For private concerns\n\n### Troubleshooting Guide\n\nSee [Computing Page](../computing.qmd#troubleshooting) for common issues.\n\n::: {.callout-tip}\n## Pro Tip\nGoogle error messages. Stack Overflow has solved most R problems.\n:::\n\n---\n\n## Key Takeaways\n\n1. **Reproducibility** is non-negotiable — renv + Git + Quarto\n\n2. **DeclareDesign** lets us simulate designs *before* collecting data\n\n3. **Diagnose before you analyze** — Power, bias, coverage\n\n4. **Lab 0** verifies your setup works — Do it early!\n\n---\n\n## For Next Time\n\n### Session 0.3: Potential Outcomes & DAGs\n\n**Readings:**\n\n- [Chernozhukov Ch. 2 (finish)](http://chapters.causalml-book.org/CausalML_chap_2.pdf)\n- [Hernán & Robins Ch. 1-2](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)\n\n**Prepare:**\n\n- Complete Lab 0\n- Bring questions about setup\n\n::: {.notes}\nEmphasize that Lab 0 must be done before Session 0.3. We won't spend class time on setup issues.\n:::\n\n---\n\n## Questions? {.center}\n\nOffice Hours: Wednesday 2-4pm\n\nSlack: #hpm883-help\n\n::: {.notes}\nOpen floor for questions. This is usually when students reveal confusion about setup.\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}